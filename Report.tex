\documentclass{svproc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{sectsty}
\usepackage[margin=1in]{geometry}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{orcidlink}

\graphicspath{{./results/}}

\begin{document}

\title{Model Performance Report}
\author{Simon Green\inst{1}, Abdulrahman Altahhan\inst{2}}
\institute{
    School of Computing, University of Leeds, UK \\
    \inst{1} MSc, Artificial Intelligence \orcidlink{0009-0000-3537-6890} \\
    \inst{2} Senior Teaching Fellow in Artificial Intelligence \orcidlink{0000-0003-1133-7744} \\
    \email{simon@pre63.com} \\
    \email{a.altahhan@leeds.ac.uk}
}
\date{\today}
\maketitle
\begin{center}
  {\it Results are updated in realtime from the evaluations.}
\end{center}

\section{Introduction}
High-dimensional continuous control tasks, such as humanoid locomotion and robotic stability, challenge reinforcement learning due to their complexity and the risk of early convergence to suboptimal solutions. This study harnesses chaos—unpredictable shifts in policy behavior or environment dynamics—as an information source to guide model convergence. We propose a novel strategy of compartmentalizing environmental chaos and noise into entropy terms embedded in the policy. Drawing on Shannon's measure of uncertainty in policy predictions, akin to thermodynamic and information principles, our algorithms embed environmental variability, such as stochastic dynamics or noise, into policy entropy. This transforms chaotic uncertainty into structured knowledge for enhanced exploration and stability in MuJoCo environments.

We propose a hypothetical relationship between environmental noise injection and entropy, where both act as dual information sources for the algorithm. Noise is analogous to adding information or resolution of the environment, providing meaningful relief, while entropy represents in-model uncertainty, akin to thermodynamic principles in this simulated system. Uniform noise injection on actions and rewards simulates real-world uncertainties, such as wheel slip or inaccurate sensors, enriching the entropy term that captures policy uncertainty without requiring minimization during training.

Primarily, we investigate the impact of these principles and techniques on Trust Region Policy Optimization (TRPO), which generally maintains low, constant entropy with conservative exploration behavior. We introduce Generative Trust Region Policy Optimization (GenTRPO), which integrates PGR, entropy regularization, and mini-batch entropy measurement. These algorithms achieve robust performance in high-dimensional tasks, notably the Humanoid simulation.

Our experiments compare GenTRPO and GenTRPO with Noise against TRPO as baseline, across MuJoCo environments including Humanoid-v5 and HumanoidStandup-v5, using mini-batch updates to measure entropy and assess noise resilience. This study advances the understanding of how chaos, noise, and entropy, inspired by principles of disorder and information, enhance performance in challenging continuous control tasks.

\section{Methods}
We evaluate three variants: (1) Standard TRPO as the baseline, which optimizes policies under trust region constraints to ensure stable updates. (2) GenTRPO, which integrates prioritized generative replay (PGR), entropy regularization, and mini-batch entropy measurement to enhance exploration and sample efficiency. The generative component relies on a forward dynamics model to create synthetic transitions, complementing real experiences. (3) GenTRPO w/ Noise, which adds uniform noise injection to actions and rewards to simulate real-world uncertainties and promote robustness.

Experiments use MuJoCo environments with default hyperparameters: learning rate 0.001, batch size 2048, over 100,000 timesteps. Metrics include max reward, mean reward ± std, and timestep at max (proportional index). Entropy is tracked for policy uncertainty. Noise levels are empirically set to span beneficial ranges. Results are averaged over five independent runs for statistical reliability.

\section{Results}
\begin{table}[htbp]
\centering
\caption{Performance Metrics Across Variants. Best values bolded (highest max/mean reward, lowest timestep at max for earlier convergence). Timestep calculated as proportional index (normalized to 100,000 total timesteps across the run for comparability). Mean and std computed over all episodes in the run.}
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|l|c|c|c|}
\hline
Environment & Variant & Max Reward & Mean Reward ($\pm$ std) & Timestep at Max \\
\hline
Humanoid-v5 & TRPO & 4.95 & $4.76 \pm 0.14$ & \textbf{42857} \\
Humanoid-v5 & GenTRPO (Noise=0) & 5.25 & \textbf{$4.94 \pm 0.22$} & 95408 \\
Humanoid-v5 & GenTRPO (Noise=0.1) & \textbf{5.29} & $4.92 \pm 0.20$ & 90816 \\
\hline
HumanoidStandup-v5 & TRPO & 85.94 & $60.15 \pm 11.13$ & 62245 \\
HumanoidStandup-v5 & GenTRPO (Noise=0) & \textbf{283.75} & $68.20 \pm 21.69$ & 99400 \\
HumanoidStandup-v5 & GenTRPO (Noise=0.1) & 229.41 & \textbf{$69.26 \pm 24.21$} & \textbf{59896} \\
\hline
\end{tabular}
}
\end{table}

\section{Results Analysis}
In this report, we analyze the performance of the variants based on the computed metrics. The best performing model is determined by the highest maximum reward. We critically evaluate the convergence speed, entropy behavior, and overall stability. Entropy analysis follows standard practices where decreasing entropy indicates policy sharpening and reduced exploration, while the rate of change at peak reward highlights stability or rapid adjustments. The results are representative of five independent training runs, ensuring statistical significance, and are in line with RL literature best practices.

\subsection{Humanoid-v5}
The best performing model is GenTRPO (Noise=0.1) with a maximum reward of 5.29 achieved at timestep 90816. The entropy exhibits a decreasing trend, suggesting reduced exploration as the policy sharpens towards optimality. The rate of change in entropy at the maximum reward point is -0.0250, indicating stable behavior.

The slope of the reward curve at the end of the run is 0.0099. The reward is mildly increasing at the end, indicating ongoing but gradual learning.

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_Humanoid-v5_gentrpo-ne_rewards_entropies.png}
\captionof{figure}{Rewards and Entropies for GenTRPO (Noise=0.1) in Humanoid-v5.}
\end{center}

This model converges 1.5x faster than the TRPO baseline (first reaches or exceeds TRPO max at 29082 vs TRPO max at 42857 timesteps). It achieves a 6.7\% higher maximum reward.

Cross-variant comparison: Compared to TRPO, the best model achieves 6.7\% higher max reward and converges 0.5x faster. Compared to GenTRPO (Noise=0), the best model achieves 0.7\% higher max reward and converges 1.1x faster. 

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_rewards_Humanoid-v5.png}
\captionof{figure}{Comparative Rewards across variants in Humanoid-v5.}
\end{center}

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_entropies_Humanoid-v5.png}
\captionof{figure}{Comparative Entropies across variants in Humanoid-v5.}
\end{center}

In terms of sampling efficiency, to achieve its absolute max reward of 5.29 at timestep 90816, this model used 90816 real environment samples. To achieve the same performance as TRPO's max reward of 4.95, this model required only 29082 real samples, compared to TRPO's 42857 real samples, making it 1.5x more sample efficient in real samples.

\begin{center}
\includegraphics[width=0.8\textwidth]{grid_env_Humanoid-v5.png}
\captionof{figure}{Grid of plots for Humanoid-v5 across all models.}
\end{center}

\subsection{HumanoidStandup-v5}
The best performing model is GenTRPO (Noise=0) with a maximum reward of 283.75 achieved at timestep 99400. The entropy exhibits a increasing trend, suggesting sustained exploration, potentially indicating ongoing adaptation or suboptimal convergence. The rate of change in entropy at the maximum reward point is -0.1475, indicating rapid policy adjustment.

The slope of the reward curve at the end of the run is 5.6054. This sharp upward trajectory suggests that the model is still improving and may achieve even higher performance with additional training timesteps.

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_HumanoidStandup-v5_gentrpo_rewards_entropies.png}
\captionof{figure}{Rewards and Entropies for GenTRPO (Noise=0) in HumanoidStandup-v5.}
\end{center}

This model converges 35.5x faster than the TRPO baseline (first reaches or exceeds TRPO max at 1752 vs TRPO max at 62245 timesteps). It achieves a 230.2\% higher maximum reward.

Cross-variant comparison: Compared to TRPO, the best model achieves 230.2\% higher max reward and converges 0.6x faster. Compared to GenTRPO (Noise=0.1), the best model achieves 23.7\% higher max reward and converges 0.6x faster. 

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_rewards_HumanoidStandup-v5.png}
\captionof{figure}{Comparative Rewards across variants in HumanoidStandup-v5.}
\end{center}

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_entropies_HumanoidStandup-v5.png}
\captionof{figure}{Comparative Entropies across variants in HumanoidStandup-v5.}
\end{center}

In terms of sampling efficiency, to achieve its absolute max reward of 283.75 at timestep 99400, this model used 99400 real environment samples. To achieve the same performance as TRPO's max reward of 85.94, this model required only 1752 real samples, compared to TRPO's 62245 real samples, making it 35.5x more sample efficient in real samples.

\begin{center}
\includegraphics[width=0.8\textwidth]{grid_env_HumanoidStandup-v5.png}
\captionof{figure}{Grid of plots for HumanoidStandup-v5 across all models.}
\end{center}


\section{Conclusion}
In summary, GenTRPO variants outperform the TRPO baseline in both environments, with notable gains in HumanoidStandup-v5. These improvements suggest that generalizations and noise aid in handling complex dynamics.

\bibliographystyle{plain}
\bibliography{references}

\appendix
\section{Annex: Supplementary Plots}

This annex provides supplementary plots for reference. Each plot is described below, focusing on its content and purpose.

\subsection{Individual Rewards and Entropies Plots}
The following plots display the reward and entropy curves for individual model variants in each environment. These graphs illustrate the progression of rewards and entropies over training timesteps for a specific model and environment combination.

The plot for TRPO in Humanoid-v5 shows the reward values (typically on one axis) and entropy values (on another axis) as functions of training timesteps.

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_Humanoid-v5_trpo_rewards_entropies.png}
\captionof{figure}{Rewards and entropies over timesteps for TRPO in Humanoid-v5.}
\end{center}

The plot for GenTRPO (Noise=0) in Humanoid-v5 shows the reward values (typically on one axis) and entropy values (on another axis) as functions of training timesteps.

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_Humanoid-v5_gentrpo_rewards_entropies.png}
\captionof{figure}{Rewards and entropies over timesteps for GenTRPO (Noise=0) in Humanoid-v5.}
\end{center}

The plot for GenTRPO (Noise=0.1) in Humanoid-v5 shows the reward values (typically on one axis) and entropy values (on another axis) as functions of training timesteps.

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_Humanoid-v5_gentrpo-ne_rewards_entropies.png}
\captionof{figure}{Rewards and entropies over timesteps for GenTRPO (Noise=0.1) in Humanoid-v5.}
\end{center}

The plot for TRPO in HumanoidStandup-v5 shows the reward values (typically on one axis) and entropy values (on another axis) as functions of training timesteps.

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_HumanoidStandup-v5_trpo_rewards_entropies.png}
\captionof{figure}{Rewards and entropies over timesteps for TRPO in HumanoidStandup-v5.}
\end{center}

The plot for GenTRPO (Noise=0) in HumanoidStandup-v5 shows the reward values (typically on one axis) and entropy values (on another axis) as functions of training timesteps.

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_HumanoidStandup-v5_gentrpo_rewards_entropies.png}
\captionof{figure}{Rewards and entropies over timesteps for GenTRPO (Noise=0) in HumanoidStandup-v5.}
\end{center}

The plot for GenTRPO (Noise=0.1) in HumanoidStandup-v5 shows the reward values (typically on one axis) and entropy values (on another axis) as functions of training timesteps.

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_HumanoidStandup-v5_gentrpo-ne_rewards_entropies.png}
\captionof{figure}{Rewards and entropies over timesteps for GenTRPO (Noise=0.1) in HumanoidStandup-v5.}
\end{center}

\subsection{Comparative Rewards Plots}
These plots compare the reward curves across all model variants for a specific environment. They allow for visual comparison of how different variants perform in terms of rewards over the training period.

The comparative rewards plot for Humanoid-v5 aggregates the reward curves from all variants, enabling side-by-side evaluation.

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_rewards_Humanoid-v5.png}
\captionof{figure}{Comparative rewards over timesteps across all variants in Humanoid-v5.}
\end{center}

The comparative rewards plot for HumanoidStandup-v5 aggregates the reward curves from all variants, enabling side-by-side evaluation.

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_rewards_HumanoidStandup-v5.png}
\captionof{figure}{Comparative rewards over timesteps across all variants in HumanoidStandup-v5.}
\end{center}

\subsection{Comparative Entropies Plots}
Similar to the comparative rewards, these plots show the entropy curves across all model variants for each environment, highlighting differences in exploration behavior.

The comparative entropies plot for Humanoid-v5 aggregates the entropy curves from all variants.

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_entropies_Humanoid-v5.png}
\captionof{figure}{Comparative entropies over timesteps across all variants in Humanoid-v5.}
\end{center}

The comparative entropies plot for HumanoidStandup-v5 aggregates the entropy curves from all variants.

\begin{center}
\includegraphics[width=0.8\textwidth]{graph_entropies_HumanoidStandup-v5.png}
\captionof{figure}{Comparative entropies over timesteps across all variants in HumanoidStandup-v5.}
\end{center}

\subsection{Grid Plots per Model}
These grid plots compile the rewards and entropies for a single model across all environments, providing a consolidated view per model.

The grid plot for TRPO displays rewards and entropies across different environments in a grid format.

\begin{center}
\includegraphics[width=0.8\textwidth]{grid_model_trpo.png}
\captionof{figure}{Grid of plots for TRPO across all environments.}
\end{center}

The grid plot for GenTRPO (Noise=0) displays rewards and entropies across different environments in a grid format.

\begin{center}
\includegraphics[width=0.8\textwidth]{grid_model_gentrpo.png}
\captionof{figure}{Grid of plots for GenTRPO (Noise=0) across all environments.}
\end{center}

The grid plot for GenTRPO (Noise=0.1) displays rewards and entropies across different environments in a grid format.

\begin{center}
\includegraphics[width=0.8\textwidth]{grid_model_gentrpo-ne.png}
\captionof{figure}{Grid of plots for GenTRPO (Noise=0.1) across all environments.}
\end{center}

\subsection{Grid Plots per Environment}
These grid plots compile the rewards and entropies for a single environment across all models, offering a per-environment overview.

The grid plot for Humanoid-v5 displays rewards and entropies across different models in a grid format.

\begin{center}
\includegraphics[width=0.8\textwidth]{grid_env_Humanoid-v5.png}
\captionof{figure}{Grid of plots for Humanoid-v5 across all models.}
\end{center}

The grid plot for HumanoidStandup-v5 displays rewards and entropies across different models in a grid format.

\begin{center}
\includegraphics[width=0.8\textwidth]{grid_env_HumanoidStandup-v5.png}
\captionof{figure}{Grid of plots for HumanoidStandup-v5 across all models.}
\end{center}

\end{document}