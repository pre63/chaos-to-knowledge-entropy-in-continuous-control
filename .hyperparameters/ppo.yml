# Tuned for PPO
HalfCheetah-v5:
  normalize: true
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e6
  batch_size: 64
  n_steps: 512
  gamma: 0.98
  learning_rate: 2.0633e-05
  ent_coef: 0.000401762
  clip_range: 0.1
  n_epochs: 20
  gae_lambda: 0.92
  max_grad_norm: 0.8
  vf_coef: 0.58096
  policy_kwargs:
    log_std_init: -2
    ortho_init: False
    activation_fn: torch.nn.ReLU
    net_arch: medium

# Tuned for PPO
Hopper-v5:
  normalize: true
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e6
  batch_size: 32
  n_steps: 512
  gamma: 0.999
  learning_rate: 9.80828e-05
  ent_coef: 0.00229519
  clip_range: 0.2
  n_epochs: 5
  gae_lambda: 0.99
  max_grad_norm: 0.7
  vf_coef: 0.835671
  policy_kwargs:
    log_std_init: -2
    ortho_init: False
    activation_fn: torch.nn.ReLU
    net_arch: medium

# Tuned for PPO
Humanoid-v5:
  normalize: true
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e7
  batch_size: 256
  n_steps: 512
  gamma: 0.95
  learning_rate: 3.56987e-05
  ent_coef: 0.00238306
  clip_range: 0.3
  n_epochs: 5
  gae_lambda: 0.9
  max_grad_norm: 2
  vf_coef: 0.431892
  policy_kwargs:
    log_std_init: -2
    ortho_init: False
    activation_fn: torch.nn.ReLU
    net_arch: medium

# Tuned for PPO
HumanoidStandup-v5:
  normalize: true
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e7
  batch_size: 32
  n_steps: 512
  gamma: 0.99
  learning_rate: 2.55673e-05
  ent_coef: 3.62109e-06
  clip_range: 0.3
  n_epochs: 20
  gae_lambda: 0.9
  max_grad_norm: 0.7
  vf_coef: 0.430793
  policy_kwargs:
    log_std_init: -2
    ortho_init: False
    activation_fn: torch.nn.ReLU
    net_arch: medium

# Tuned for PPO
Reacher-v5:
  normalize: true
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e6
  batch_size: 32
  n_steps: 512
  gamma: 0.9
  learning_rate: 0.000104019
  ent_coef: 7.52585e-08
  clip_range: 0.3
  n_epochs: 5
  gae_lambda: 1.0
  max_grad_norm: 0.9
  vf_coef: 0.950368

# Tuned for PPO
Swimmer-v5:
  normalize: true
  n_envs: 4
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  batch_size: 256
  n_steps: 1024
  gamma: 0.9999
  learning_rate: !!float 6e-4
  gae_lambda: 0.98
  clip_range: 0.2
  n_epochs: 10
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5