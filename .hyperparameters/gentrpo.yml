Pendulum-v1:
  policy: 'MlpPolicy'
  n_timesteps: 100000
  reward_threshold: -3.0

LunarLanderContinuous-v3:
  policy: 'MlpPolicy'
  n_timesteps: 100000
  reward_threshold: 195

Ant-v5:
  policy: 'MlpPolicy'
  n_timesteps: 100000
  reward_threshold: 1000

Humanoid-v5:
  policy: 'MlpPolicy'
  n_timesteps: 100000
  n_steps: 512
  gamma: 0.95
  learning_rate: 0.03648542602737527
  n_critic_updates: 20
  cg_max_steps: 5
  target_kl: 0.03
  gae_lambda: 0.95
  batch_size: 2048
  net_arch: medium
  activation_fn: tanh
  entropy_coef: -0.59
  sampling_coef: -0.69
  buffer_capacity: 30000
  epsilon: 0.15000000000000002
  orthogonal_init: 0
  n_envs: 6
  normalize_advantage: 1

InvertedDoublePendulum-v5:
  policy: 'MlpPolicy'
  n_timesteps: 100000
  reward_threshold: 410

RocketLander-v0:
  policy: 'MlpPolicy'
  n_timesteps: 100000
  reward_threshold: -3.0

HalfCheetah-v5:
  policy: 'MlpPolicy'
  n_timesteps: 100000
  n_steps: 16
  gamma: 0.95
  learning_rate: 0.0006115378553216117
  n_critic_updates: 25
  cg_max_steps: 10
  target_kl: 0.02
  gae_lambda: 1
  batch_size: 1024
  net_arch: large
  activation_fn: tanh
  entropy_coef: 0.49
  sampling_coef: 0.14000000000000012
  epsilon: 0.7000000000000001
  orthogonal_init: 1
  n_envs: 8
  normalize_advantage: 1

Hopper-v5: 
  policy: 'MlpPolicy'
  n_timesteps: 100000
  n_steps: 128
  gamma: 0.99
  learning_rate: 0.0008758107610955626
  n_critic_updates: 30
  cg_max_steps: 25
  target_kl: 0.01
  gae_lambda: 0.92
  batch_size: 2048
  net_arch: small
  activation_fn: relu
  entropy_coef: -0.22999999999999998
  sampling_coef: -0.47
  epsilon: 0.45000000000000007
  orthogonal_init: 1
  n_envs: 4
  normalize_advantage: 1

HumanoidStandup-v5:
  policy: 'MlpPolicy'
  n_timesteps: 100000

InvertedPendulum-v5:
  policy: 'MlpPolicy'
  n_timesteps: 100000

Pusher-v5:
  policy: 'MlpPolicy'
  n_timesteps: 100000

Reacher-v5:
  policy: 'MlpPolicy'
  n_timesteps: 100000

Swimmer-v5:
  policy: 'MlpPolicy'
  n_timesteps: 100000

Walker2d-v5:
  policy: 'MlpPolicy'
  n_timesteps: 100000

